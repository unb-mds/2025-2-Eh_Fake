# üìÑ DOCUMENTA√á√ÉO T√âCNICA DO SISTEMA DE MINERA√á√ÉO DE NOT√çCIAS

## 1. Vis√£o Geral e Arquitetura

O sistema implementa um *pipeline* modularizado de **Minera√ß√£o e Classifica√ß√£o de Dados** para coletar artigos de m√∫ltiplos *publishers* (sites de tecnologia, grandes portais e fontes sensacionalistas). A arquitetura separa as responsabilidades:

1.  **Coleta (*minerador.py*):** Realiza o *web crawling* inteligente para obter os dados brutos.
2.  **Processamento (*processor\_b.py*):** Padroniza o esquema de dados para o Banco de Dados (BD).

***

## 2. M√≥dulo de Coleta: `minerador.py`

Este m√≥dulo √© o *crawler* principal, respons√°vel por navegar nos sites e extrair o conte√∫do bruto.

### 2.1. Estrat√©gia de Rastreamento (DFS Controlado)

O m√≥dulo utiliza a t√©cnica de **Busca em Profundidade (DFS) Controlada** para maximizar a cobertura semanal, sem desperdi√ßar recursos.

* **Profundidade (`max_depth`):** Limita o rastreamento a URLs de √≠ndice ou categorias (N√≠vel 2 ou 3), conforme definido em cada fun√ß√£o de *scraper*.
* **Fila de Navega√ß√£o:** Gerencia URLs de **categorias/√≠ndice** (`urls_to_crawl`) para garantir que o *crawler* visite apenas p√°ginas que listam artigos (e n√£o os artigos finais).

### 2.2. Par√¢metros Globais Chave

| Constante | Prop√≥sito | Tipo | Exemplo |
| :--- | :--- | :--- | :--- |
| `DAYS_TO_LOOK_BACK` | Define o intervalo de busca (e.g., 3 para hoje + 2 dias). | `int` | 3 |
| `SITE_MAP` | Dicion√°rio de configura√ß√£o de execu√ß√£o (URLs e fun√ß√µes de *scraper*). | `dict` | G1, TECMUNDO, OFUXICO |

### 2.3. Heur√≠stica de Identifica√ß√£o de Artigo

Para decidir se uma URL deve ser **coletada (artigo final)** ou **seguida (categoria)**, o *scraper* utiliza uma **heur√≠stica de contagem de barras** (`/`) na URL para **diferenciar um *link de artigo* de um *link de categoria***.

---

## 3. M√≥dulo de Processamento: `processor_b.py`

Este m√≥dulo transforma os dados JSON brutos (salvos pelo `minerador.py`) em um formato padronizado e limpo, pronto para a ingest√£o dos dados na pasta `./noticias/finais/`.

### 3.1. Esquema de Conformidade do Banco de Dados

O *processor* garante que cada arquivo JSON final possua exatamente as 10 colunas definidas como o **contrato** para a tabela de destino do BD:

| Coluna BD | Origem / L√≥gica | Prop√≥sito |
| :--- | :--- | :--- |
| **`title`**, **`description`** | Mapeamento direto do JSON. | Informa√ß√µes b√°sicas do artigo. |
| **`maintext`** | Mapeamento do campo `maintext`. | **Corpo do conte√∫do para an√°lise.** |
| **`link`**, `imageSrc` | Mapeamento de `url` e `image_url`. | Chave de refer√™ncia e imagem. |
| **`source`** | Extra√≠do da subpasta de origem (e.g., 'G1'). | Metadado de origem. |
| **`created_at`** | `datetime.now()` | Carimbo de data/hora do processamento. |
| **`status`** | **`'Error'`** (Valor Padr√£o) | Conformidade com o tipo `ENUM` do BD (para classifica√ß√£o posterior). |
| **`confidence`** | **`0.00`** (Valor Padr√£o) | Conformidade com o tipo `NUMERIC` do BD. |

### 3.2. L√≥gica de Valida√ß√£o e Conformidade

* **Preenchimento de Seguran√ßa:** O m√≥dulo preenche os campos `status` com o valor **`'Error'`** e `confidence` com **`0.00`** em todos os registros, prevenindo erros de viola√ß√£o de tipo na inser√ß√£o SQL.
* **Filtro de Leitura:** Garante que os arquivos j√° processados na pasta `finais` **n√£o sejam lidos novamente**, utilizando a vari√°vel `EXCLUDE_FOLDER_NAME`.

---

## 4. Garantia de Qualidade (Testes de Unidade)

O projeto √© validado por um sistema de **Testes de Unidade** (`test_minerador.py`) que garante a integridade da l√≥gica do sistema atrav√©s de simula√ß√µes (*Mocks*):

1.  **Integridade do Scraper:** Verifica se as fun√ß√µes de raspagem (`scrape_site_links`) conseguem identificar e retornar os links corretos a partir de um HTML simulado.
2.  **Conformidade do Esquema:** Assegura que o **`processor\_b.py`** adere rigidamente ao `BD_FIELDNAMES`, verificando a presen√ßa de todos os campos e a aplica√ß√£o dos valores de seguran√ßa (`'Error'`, `0.00`) para o BD.